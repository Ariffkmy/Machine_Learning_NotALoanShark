Overview: Flow of programs:

Step 1: labelencoder.py -> Data cleaned up
Step 2: simplestats.py -> Basic statistic
Step 3: eda.py -> EDA task
Step 4: combinemodel.py -> Model training, building and testing

==================================================
# Step 1)  Data cleaning 
==================================================

Original dataset: loan_default_data.csv

1) Removing some unrelevant row (first row) and columns ( issue_date,zip_code,address,earlist_creadit_line,last_payment_amnt,last_payment_date,next_payment,last_credit_pull_date)
   Reason: Irrelevant dataset could affect the machine learning model process. To know whether the column is relevant or not, at this point of assessment, I have to 
   consider with my logic. In the real world, there ll be few stakeholders that should decided on the data relevancy.

2) Convert non numerical column values to numerical by using labelencoder method - program name to do conversion: labelencoder.py.
   Reason: First reason is machine learning model trained better with numerical data compare with mixture of numerical and non numerical. Second reason will be,
   this conversion is done initially instead of together during EDA is I want to avoid the python program that run EDA task get overloaded. This conversion task
   itself already take some time to process.
    2.1) This will create a new file name encoded_dataset.csv
    2.2) Encoded columns : 'term', 'employment_length', 'home_ownership','verification_status','loan_status','purpose'
         Reason of doing this:  
    2.3) Mapping for term:
            36 months --> 0
            60 months --> 1

            Mapping for employment_length:
            1 year --> 0
            10+ years --> 1
            2 years --> 2
            3 years --> 3
            4 years --> 4
            5 years --> 5
            6 years --> 6
            7 years --> 7
            8 years --> 8
            9 years --> 9
            < 1 year --> 10
            nan --> 11

            Mapping for home_ownership:
            MORTGAGE --> 0
            NONE --> 1
            OTHER --> 2
            OWN --> 3
            RENT --> 4

            Mapping for verification_status:
            Not Verified --> 0
            Source Verified --> 1
            Verified --> 2

            Mapping for loan_status:
            Charged Off --> 0
            Current --> 1
            Default --> 2
            Does not meet the credit policy. Status:Charged Off --> 3
            Does not meet the credit policy. Status:Fully Paid --> 4
            Fully Paid --> 5
            In Grace Period --> 6
            Late (16-30 days) --> 7
            Late (31-120 days) --> 8

            Mapping for purpose:
            car --> 0
            credit_card --> 1
            debt_consolidation --> 2
            educational --> 3
            home_improvement --> 4
            house --> 5
            major_purchase --> 6
            medical --> 7
            moving --> 8
            other --> 9
            renewable_energy --> 10
            small_business --> 11
            vacation --> 12
            wedding --> 13

        2.4) Changing percentage value to decimal

Hence, the cleaned dataset to be used for training and testing is encoded_dataset.csv.

==================================================
# Step 2) Basic statistic and analysis on the data
==================================================
1) To know on the count, mean,std, min, 25% percentile,50% percentile, 75% percentile
& max count : program to run this task - simplestats.py


==================================================
# Step 3) Perform EDA - Exploratory Data Analysis
==================================================

1) Program that performing EDA task: eda.py

2) In this EDA, there are a few things have been achieved:

    2.1) Univariate Analysis:
            -Target Variable Analysis: Analyze the distribution of the target variable.
            -Feature Analysis: Analyze the distribution of each feature (mean, median, mode, standard deviation, histograms, box plots).

    2.2) Bivariate Analysis:

            -Target vs Feature: Analyze the relationship between the target variable and each feature (scatter plots, box plots, bar plots for categorical variables, correlation for numerical variables).

    2.3) Multivariate Analysis:

            -Feature Relationships: Analyze relationships between multiple features (pair plots, correlation matrix, heatmaps).

    2.4) Feature Engineering:

            -Create New Features: Based on domain knowledge and initial insights.
            -Transform Features: Scaling, normalization, encoding categorical variables.

3) Reminder, if you run this eda.py program, you have to go throug all diagrams because I prepare the logic 
   to show box plot, histograms, Scatter plots, Correlation matrix for columns in the dataset - Pair Plot (selected first 5 columns). Just click close icon
   to navigate to next diagrams.



==================================================
# Step 4) Model training and key metrics
==================================================

1) I have selected 3 models to carry out the task which are:
    1.1) Decision Tree Regressor
    1.2) Random Forest Regressor
    1.3) Linear Regression

Reason: The given dataset is having a target variable, which is repay_fail. This means, there will be other features columns that will be used
to do the prediction in resulting the target variable. Hence, generally and commonly, there will be 3 common models that suits to this requirement which are
the one I mentioned above.

2) Program to do the training/building the model: combinemodel.py

3) Evaluation on key metrics for each model:
    3.1) There will be 2 selected key metrics to be used when evaluating the performance of these 3 models:
            3.1.1) Mean Squared Error (MSE)
            3.1.2) R-squared (RÂ²)
    
    Reasons of selection: Apart from commonly used in prediction model, the other main reason I choose these 2 are
    the sensitivity to error and comparison to baseline. To explain more, the dataset given is having a lot of variance in determining
    the target variable, this type of data need to have high sensitivity to error in order to give accurate prediction.

4) Which model is performed better:
To know which is better, we rely on the 2 main metrics. MSE that having lower values are better and 
R2 that is having higher values are better

In the combinemodel.py, I did print some result when I tested with a row of new data.
Based on the result, we do the comparison where the result of the run is:

Decision Tree Regression Performance:
Training set Mean Squared Error (MSE): 0.0
Test set Mean Squared Error (MSE): 0.00012993762993762994
Training set R-squared (R2): 1.0
Test set R-squared (R2): 0.9989969845675597

Random Forest Regression Performance:
Training set Mean Squared Error (MSE): 2.3259591332878537e-06
Test set Mean Squared Error (MSE): 4.142411642411643e-05
Training set R-squared (R2): 0.9999818655254802
Test set R-squared (R2): 0.999680238680138

Linear Regression Performance:
Training set Mean Squared Error (MSE): 0.01866071568078792
Test set Mean Squared Error (MSE): 0.018587089121568152
Training set R-squared (R2): 0.8545106540383076
Test set R-squared (R2): 0.8565224158542498
 
Tested with new data:

Predicted_repay_fail_RF : 0.0
Predicted_repay_fail_RF: 0.0 
Predicted_repay_fail_LR: 0.2

Thus, we can conclude that Random Forest Regression perform better compare to the other 2 because
is has the lowest test MSE and highest test of R2.


